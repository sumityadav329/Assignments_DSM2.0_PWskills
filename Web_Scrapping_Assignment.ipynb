{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
        "\n",
        "## A1.\n",
        "\n",
        "**Web scraping is the process of extracting data from websites. It involves sending HTTP requests to web pages, parsing the HTML or other structured data on those pages, and then extracting the desired information. Web scraping is used to gather data from the internet for a variety of purposes. It's a valuable tool for accessing and collecting data from websites that don't provide structured APIs for data retrieval.**\n",
        "\n",
        "Web scraping is used for various purposes, including:\n",
        "\n",
        "1. **Data Collection and Analysis:** Web scraping is often used to gather data for analysis. For example, businesses can scrape e-commerce websites to collect product pricing and availability data, market researchers can collect social media data for sentiment analysis, and news organizations can scrape various sources for articles and headlines.\n",
        "\n",
        "2. **Competitive Intelligence:** Many businesses use web scraping to gain insights into their competitors. They can track competitors' product offerings, pricing strategies, and customer reviews. This data helps businesses make informed decisions and adjust their strategies.\n",
        "\n",
        "3. **Research and Monitoring:** In research, web scraping can be used to collect data for academic or scientific purposes. For instance, researchers may scrape data from academic publications, government websites, or social media to study trends, conduct surveys, or monitor changes in certain domains.\n",
        "\n",
        "Here are three specific areas where web scraping is commonly used to gather data:\n",
        "\n",
        "1. **E-commerce and Price Comparison:** Retailers often scrape competitors' websites to monitor product prices, stock levels, and customer reviews. This data helps them adjust their pricing strategies and make informed decisions about which products to stock.\n",
        "\n",
        "2. **Content Aggregation and News:** News aggregators use web scraping to collect headlines, articles, and multimedia content from various news sources. They can provide users with a centralized location to access news from multiple publishers.\n",
        "\n",
        "3. **Real Estate and Property Listings:** Companies and individuals in the real estate industry use web scraping to gather property listings from multiple sources. This data includes property details, prices, and location information, which can be used for market analysis and property research.\n",
        "\n"
      ],
      "metadata": {
        "id": "5cbJXktqL0Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What are the different methods used for Web Scraping?\n",
        "## A2.\n",
        "Web scraping can be accomplished using a variety of methods and tools, ranging from simple manual techniques to more sophisticated automated approaches. Here are some common methods used for web scraping:\n",
        "\n",
        "1. **Manual Copy-Paste:** The most basic form of web scraping involves manually selecting and copying data from a web page and then pasting it into a local document. This method is suitable for very small amounts of data but is not efficient for large-scale scraping.\n",
        "\n",
        "2. **Regular Expressions:** Regular expressions (regex) can be used to extract specific patterns or data from HTML or text content. This method is more efficient than manual copy-paste but can be complex, error-prone, and not suitable for parsing complex HTML structures.\n",
        "\n",
        "3. **HTML Parsing with Libraries:** Programming languages like Python offer libraries for parsing HTML, such as BeautifulSoup and lxml. These libraries allow you to navigate and extract data from HTML documents efficiently. They are popular choices for web scraping.\n",
        "\n",
        "4. **Headless Browsers:** Headless browsers like Puppeteer (for JavaScript) and Selenium (for various languages) automate the process of interacting with websites. They can render web pages, execute JavaScript, and extract data from dynamically generated web pages. These tools are often used for scraping data from websites with a heavy reliance on JavaScript.\n",
        "\n",
        "5. **APIs:** Some websites provide Application Programming Interfaces (APIs) that allow developers to access structured data directly. APIs are a preferred method for data retrieval, as they offer a more stable and structured way to get information. Web scraping is often a fallback when an API is not available or lacks necessary data.\n",
        "\n",
        "6. **Scraping Frameworks:** There are web scraping frameworks and tools specifically designed for web scraping tasks, such as Scrapy (a Python-based framework) and Octoparse. These tools offer built-in features for navigating websites and extracting data efficiently.\n",
        "\n",
        "7. **Data Extraction Tools:** Commercial data extraction tools like Import.io, ParseHub, and WebHarvy provide a user-friendly interface to scrape data from websites without requiring extensive programming skills.\n",
        "\n",
        "8. **Crawlers and Spiders:** Web crawling is a broader activity where automated bots (crawlers or spiders) systematically navigate the web, visit multiple web pages, and collect data. Search engines like Google use web crawling to index the web, and you can create your own crawlers for specific data collection tasks.\n",
        "\n",
        "9. **Proxy Servers and IP Rotation:** In some cases, web scraping might involve rotating IP addresses or using proxy servers to bypass rate limits, IP bans, or geographical restrictions imposed by websites. This is especially useful for large-scale scraping.\n",
        "\n",
        "10. **Cloud-Based Solutions:** Some cloud platforms offer web scraping services and tools that simplify the process of data extraction. These platforms allow you to deploy and scale your web scraping scripts in the cloud.\n",
        "\n"
      ],
      "metadata": {
        "id": "VWcZYG4_L0Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. What is Beautiful Soup? Why is it used?\n",
        "## A3.\n",
        "\n",
        "**Beautiful Soup is a Python library commonly used for web scraping. It provides tools for parsing HTML and XML documents and extracting data from them in a structured and easy-to-navigate way. Beautiful Soup is a popular choice for web scraping because it simplifies the process of working with HTML documents, allowing you to access and manipulate data efficiently.**\n",
        "\n",
        "**Beautiful Soup used for:**\n",
        "\n",
        "**1. Parsing HTML and XML:** Beautiful Soup is primarily used for parsing and navigating HTML and XML documents. These documents are the backbone of most websites, containing the structured information that is presented to users in web browsers.\n",
        "\n",
        "**2. Easy Navigation:** Beautiful Soup allows you to navigate and search the parsed HTML or XML documents using Python code. You can traverse the document's elements, search for specific tags, access attributes, and extract data, making it easier to find and collect the information you need from web pages.\n",
        "\n",
        "**3. Tree Structure:** It constructs a parse tree from the provided document, creating a hierarchical structure that mirrors the nesting of HTML or XML tags. This tree structure simplifies navigation and data extraction by enabling you to move up and down the document's hierarchy.\n",
        "\n",
        "**4. Tag and Attribute Access:** Beautiful Soup provides methods and attributes for accessing tags and their attributes easily. For instance, you can access the content within specific HTML tags, check the attributes of a tag, or extract the values of those attributes.\n",
        "\n",
        "**5. Robust Error Handling:** Beautiful Soup is designed to handle imperfect HTML or XML documents. It can parse documents with missing or mismatched tags and attributes, making it more resilient to inconsistencies in web page structures.\n",
        "\n",
        "**6. Filter and Search:** You can filter and search for specific elements in the document using a wide range of search methods and filters. For example, you can find all the links, extract text from paragraphs, or filter elements based on certain criteria.\n",
        "\n",
        "**7. Data Extraction:** Beautiful Soup simplifies the process of extracting data from web pages. You can extract text, links, images, and other content from the document, which is useful for various web scraping applications.\n",
        "\n",
        "**8. Integration with Requests:** Beautiful Soup is often used in combination with the `requests` library to retrieve web pages. You can send an HTTP request to a web page, get the HTML content, and then use Beautiful Soup to parse and extract data from that content.\n",
        "\n",
        "**9. Extensibility:** While Beautiful Soup is simple to use, it also provides extensibility for more complex tasks. You can create custom parsers or parsers for specific use cases when the built-in parsers are not sufficient.\n",
        "\n",
        "**10. Pythonic Syntax:** Beautiful Soup's syntax and method names are designed to be Pythonic and intuitive, making it easy for Python developers to work with.\n",
        "\n",
        "In summary, Beautiful Soup is a valuable tool for web scraping and data extraction. It simplifies the process of parsing and navigating HTML and XML documents, making it easier for developers to extract structured data from websites. It is widely used in web scraping projects due to its simplicity and effectiveness in handling web page content.\n",
        "\n",
        "# Example"
      ],
      "metadata": {
        "id": "ZJUONoyBL0Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Fetch the webpage content\n",
        "url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
        "response = requests.get(url)\n",
        "\n",
        "html_content = response.content\n",
        "\n",
        "# Create a Beautiful Soup object\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find all the hyperlinks in the page\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    href = link.get('href')\n",
        "    if href and href.startswith('http'):\n",
        "        links.append(href)\n",
        "\n",
        "# Print the links\n",
        "for link in links:\n",
        "    print(link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD1dduEdL0OA",
        "outputId": "ff7c87b0-1cff-4a7e-d1bf-5709d9b2f9cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
            "https://ar.wikipedia.org/wiki/%D8%AA%D8%AC%D8%B1%D9%8A%D9%81_%D9%88%D9%8A%D8%A8\n",
            "https://ca.wikipedia.org/wiki/Web_scraping\n",
            "https://cs.wikipedia.org/wiki/Web_scraping\n",
            "https://ary.wikipedia.org/wiki/%D8%AA%D8%BA%D8%B1%D8%A7%D9%81_%D9%84%D9%88%D9%8A%D8%A8\n",
            "https://de.wikipedia.org/wiki/Screen_Scraping\n",
            "https://es.wikipedia.org/wiki/Web_scraping\n",
            "https://eu.wikipedia.org/wiki/Web_scraping\n",
            "https://fa.wikipedia.org/wiki/%D9%88%D8%A8_%D8%A7%D8%B3%DA%A9%D8%B1%D9%BE%DB%8C%D9%86%DA%AF\n",
            "https://fr.wikipedia.org/wiki/Web_scraping\n",
            "https://id.wikipedia.org/wiki/Web_scraping\n",
            "https://is.wikipedia.org/wiki/Vefs%C3%B6fnun\n",
            "https://it.wikipedia.org/wiki/Web_scraping\n",
            "https://lv.wikipedia.org/wiki/Rasmo%C5%A1ana\n",
            "https://nl.wikipedia.org/wiki/Scrapen\n",
            "https://ja.wikipedia.org/wiki/%E3%82%A6%E3%82%A7%E3%83%96%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%94%E3%83%B3%E3%82%B0\n",
            "https://pt.wikipedia.org/wiki/Coleta_de_dados_web\n",
            "https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%B1-%D1%81%D0%BA%D1%80%D0%B5%D0%B9%D0%BF%D0%B8%D0%BD%D0%B3\n",
            "https://tr.wikipedia.org/wiki/Web_kaz%C4%B1ma\n",
            "https://uk.wikipedia.org/wiki/Web_scraping\n",
            "https://zh-yue.wikipedia.org/wiki/%E7%B6%B2%E9%A0%81%E5%88%AE%E6%96%99\n",
            "https://zh.wikipedia.org/wiki/%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96\n",
            "https://www.wikidata.org/wiki/Special:EntityPage/Q665452#sitelinks-wikipedia\n",
            "https://www.wikidata.org/wiki/Special:EntityPage/Q665452\n",
            "https://www.google.com/search?as_eq=wikipedia&q=%22Web+scraping%22\n",
            "https://www.google.com/search?tbm=nws&q=%22Web+scraping%22+-wikipedia&tbs=ar:1\n",
            "https://www.google.com/search?&q=%22Web+scraping%22&tbs=bkt:s&tbm=bks\n",
            "https://www.google.com/search?tbs=bks:1&q=%22Web+scraping%22+-wikipedia\n",
            "https://scholar.google.com/scholar?q=%22Web+scraping%22\n",
            "https://www.jstor.org/action/doBasicSearch?Query=%22Web+scraping%22&acc=on&wc=on\n",
            "https://en.wikipedia.org/w/index.php?title=Web_scraping&action=edit\n",
            "https://en.wikipedia.org/w/index.php?title=Web_scraping&action=edit\n",
            "https://doi.org/10.5334%2Fdsj-2021-024\n",
            "https://doi.org/10.5334%2Fdsj-2021-024\n",
            "https://www.worldcat.org/issn/1683-1470\n",
            "https://api.semanticscholar.org/CorpusID:237719804\n",
            "http://www.searchenginehistory.com/\n",
            "https://web.archive.org/web/20161011080619/https://pdfs.semanticscholar.org/4fb4/3c5a212df751e84c3b2f8d29fabfe56c3616.pdf\n",
            "https://doi.org/10.1145%2F1281192.1281287\n",
            "https://api.semanticscholar.org/CorpusID:833565\n",
            "https://pdfs.semanticscholar.org/4fb4/3c5a212df751e84c3b2f8d29fabfe56c3616.pdf\n",
            "http://www.gooseeker.com/en/node/knowledgebase/freeformat\n",
            "http://www.xconomy.com/san-francisco/2012/07/25/diffbot-is-using-computer-vision-to-reinvent-the-semantic-web/\n",
            "https://web.archive.org/web/20020308222536/http://www.chillingeffects.org/linking/faq.cgi#QID596\n",
            "http://www.chillingeffects.org/linking/faq.cgi#QID596\n",
            "http://scholarship.law.berkeley.edu/btlj/vol29/iss4/16/\n",
            "https://doi.org/10.15779%2FZ38B39B\n",
            "https://www.worldcat.org/issn/1086-3818\n",
            "http://www.tomwbell.com/NetLaw/Ch06.html\n",
            "https://web.archive.org/web/20020308222536/http://www.chillingeffects.org/linking/faq.cgi#QID460\n",
            "http://www.chillingeffects.org/linking/faq.cgi#QID460\n",
            "http://www.tomwbell.com/NetLaw/Ch07/Ticketmaster.html\n",
            "https://web.archive.org/web/20110723131832/http://www.fornova.net/documents/AAFareChase.pdf\n",
            "http://www.fornova.net/documents/AAFareChase.pdf\n",
            "https://web.archive.org/web/20160305025808/http://www.thefreelibrary.com/American+Airlines,+FareChase+Settle+Suit.-a0103213546\n",
            "http://www.thefreelibrary.com/American+Airlines,+FareChase+Settle+Suit.-a0103213546\n",
            "http://www.imperva.com/docs/WP_Detecting_and_Blocking_Site_Scraping_Attacks.pdf\n",
            "https://web.archive.org/web/20110211123854/http://library.findlaw.com/2003/Jul/29/132944.html\n",
            "http://library.findlaw.com/2003/Jul/29/132944.html\n",
            "https://web.archive.org/web/20130921054619/http://www.fornova.net/documents/Cvent.pdf\n",
            "http://www.fornova.net/documents/Cvent.pdf\n",
            "https://www.scribd.com/doc/249068700/LinkedIn-v-Resultly-LLC-Complaint?secret_password=pEVKDbnvhQL52oKfdrmT\n",
            "http://newmedialaw.proskauer.com/2014/12/05/qvc-sues-shopping-app-for-web-scraping-that-allegedly-triggered-site-outage/\n",
            "https://web.archive.org/web/20110723132015/http://www.fornova.net/documents/pblog-bna-com.pdf\n",
            "http://www.fornova.net/documents/pblog-bna-com.pdf\n",
            "https://www.techdirt.com/articles/20090605/2228205147.shtml\n",
            "https://www.eff.org/cases/facebook-v-power-ventures\n",
            "https://web.archive.org/web/20071012005033/http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf\n",
            "http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf\n",
            "http://www.bailii.org/ie/cases/IEHC/2010/H47.html\n",
            "https://web.archive.org/web/20120624103316/http://www.lkshields.ie/htmdocs/publications/newsletters/update26/update26_03.htm\n",
            "http://www.lkshields.ie/htmdocs/publications/newsletters/update26/update26_03.htm\n",
            "https://www.cnil.fr/fr/la-reutilisation-des-donnees-publiquement-accessibles-en-ligne-des-fins-de-demarchage-commercial\n",
            "https://medium.com/@finddatalab/can-you-still-perform-web-scraping-with-the-new-cnil-guidelines-bf3e20d0edc2\n",
            "https://web.archive.org/web/20191203113701/https://www.lloyds.com/~/media/5880dae185914b2487bed7bd63b96286.ashx\n",
            "https://www.lloyds.com/~/media/5880dae185914b2487bed7bd63b96286.ashx\n",
            "http://www.webstartdesign.com.au/spam_business_practical_guide.pdf\n",
            "https://s3.us-west-2.amazonaws.com/research-papers-mynk/Breaking-Fraud-And-Bot-Detection-Solutions.pdf\n",
            "https://en.wikipedia.org/w/index.php?title=Web_scraping&oldid=1179331542\n",
            "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\n",
            "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Universal_Code_of_Conduct\n",
            "https://developer.wikimedia.org\n",
            "https://stats.wikimedia.org/#/en.wikipedia.org\n",
            "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement\n",
            "https://wikimediafoundation.org/\n",
            "https://www.mediawiki.org/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Q4. Why is flask used in this Web Scraping project?\n",
        "## A4.\n",
        "\n",
        "Flask is used in a web scraping project for various reasons, depending on the specific requirements and use cases. Here are some reasons why Flask might be used in a web scraping project:\n",
        "\n",
        "1. **API for Data Retrieval:** Flask can be used to create a web application that serves as an API for data retrieval. This can be particularly useful when multiple users or systems need to access the scraped data in a structured format. Flask provides a convenient way to define routes and endpoints for data retrieval, making it easy to access the scraped data.\n",
        "\n",
        "2. **Web Interface for Data Visualization:** Flask can be used to create a web interface to visualize and interact with the scraped data. This can be especially beneficial when the data needs to be presented in a user-friendly manner, with features such as filtering, sorting, and searching. Flask's ability to serve HTML templates and interact with databases makes it a good choice for creating data dashboards.\n",
        "\n",
        "3. **Authentication and Authorization:** Flask can handle user authentication and authorization, which is crucial when controlling who can access the scraped data. If you want to restrict access to the data or provide different levels of access to different users, Flask's built-in support for user management can be beneficial.\n",
        "\n",
        "4. **Data Processing and Transformation:** Flask can be used to process and transform the scraped data before serving it to clients. For instance, you can use Flask to clean, filter, or reformat the data to make it more suitable for specific use cases.\n",
        "\n",
        "5. **Logging and Monitoring:** Flask allows you to implement logging and monitoring functionality, which is important for keeping track of web scraping tasks. You can log information about the scraping process, detect errors, and set up alerts or notifications for certain conditions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m6yWmL_ML0RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
        "## A5.\n",
        "\n",
        "Two AWS Services used in this project are:\n",
        "\n",
        "  - 1.**Elastic Beanstalk**\n",
        "  - 2.**Code Pipeline**\n",
        "\n",
        "### **1. Elastic Beanstalk**\n",
        "Elastic Beanstalk is a fully managed service provided by Amazon Web Services (AWS) that allows developers to easily deploy, manage, and scale web applications and services written in popular programming languages like Java, Python, Node.js, PHP, Ruby, Go, and .NET. With Elastic Beanstalk, developers can focus on writing code without worrying about the underlying infrastructure, as the service handles provisioning and configuration of the resources needed to run the application.\n",
        "\n",
        "Here are some key features of Elastic Beanstalk:\n",
        "\n",
        "**Platform as a Service (PaaS):** Elastic Beanstalk abstracts away the underlying infrastructure and provides a simple interface for developers to deploy their applications. Developers simply upload their application code, and Elastic Beanstalk handles the rest, including provisioning the necessary resources (such as compute instances, load balancers, and databases) and configuring the environment.\n",
        "\n",
        "**Multi-language Support:** Elastic Beanstalk supports a wide range of programming languages, frameworks, and platforms, including Java, Python, Node.js, PHP, Ruby, Go, and .NET. It also supports popular web servers like Apache, Nginx, and IIS.\n",
        "\n",
        "**Easy Deployment:** Developers can deploy their applications to Elastic Beanstalk using a variety of methods, including the Elastic Beanstalk console, the AWS CLI, or APIs. Elastic Beanstalk supports versioning of deployments, so developers can roll back to a previous version if needed.\n",
        "\n",
        "**Auto Scaling:** Elastic Beanstalk automatically scales the application up or down based on demand, ensuring that the application is always available and responsive to users. It can also automatically balance traffic across multiple instances of the application to optimize performance.\n",
        "\n",
        "**Monitoring and Logging:** Elastic Beanstalk provides monitoring and logging capabilities that allow developers to monitor the health and performance of their application, and troubleshoot issues if they arise. It also integrates with other AWS services like CloudWatch and Elastic Load Balancing to provide a complete solution for monitoring and managing applications.\n",
        "\n",
        "Overall, Elastic Beanstalk is a powerful and flexible service that can help developers quickly and easily deploy and manage web applications and services on AWS.\n",
        "\n",
        "\n",
        "### **2. Code Pipeline:**\n",
        "AWS CodePipeline is a fully managed continuous delivery service provided by Amazon Web Services (AWS). It automates the release process for applications, enabling developers to rapidly and reliably build, test, and deploy their code changes.\n",
        "\n",
        "Here are some key features of AWS CodePipeline:\n",
        "\n",
        "**Pipeline Creation:** Developers can create custom pipelines for their applications, specifying the source code repository, build tools, testing frameworks, deployment targets, and other settings. They can also define the stages of the pipeline and the actions that should be performed in each stage.\n",
        "\n",
        "**Source Code Integration:** CodePipeline integrates with a wide range of source code repositories, including AWS CodeCommit, GitHub, and Bitbucket. Developers can configure their pipelines to automatically detect code changes in the repository and trigger the build and deployment process.\n",
        "\n",
        "**Build and Test Automation:** CodePipeline supports a variety of build and test tools, including AWS CodeBuild, Jenkins, and Bamboo. Developers can configure their pipelines to run automated tests as part of the build process, ensuring that code changes meet quality standards before being deployed.\n",
        "\n",
        "**Deployment Automation:** CodePipeline can deploy applications to a wide range of targets, including Amazon EC2 instances, AWS Elastic Beanstalk environments, and AWS Lambda functions. It can also integrate with other AWS services like AWS CodeDeploy and AWS CloudFormation to support more complex deployment scenarios.\n",
        "\n",
        "**Continuous Monitoring:** CodePipeline provides continuous monitoring of the pipeline and its stages, giving developers visibility into the progress of each stage and the status of each action. It also integrates with AWS CloudWatch to provide monitoring and alerting capabilities for the pipeline and the application.\n",
        "\n",
        "Overall, AWS CodePipeline is a powerful tool for automating the release process for applications, enabling developers to deploy changes quickly and reliably while maintaining high quality standards. By eliminating the need for manual intervention and automating many of the tedious and error-prone tasks involved in software deployment, CodePipeline can help teams deliver software faster and with fewer errors."
      ],
      "metadata": {
        "id": "oJ8owNvhZxKa"
      }
    }
  ]
}