{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
        "\n",
        "**A1.**\n",
        "\n",
        "Min-Max scaling is a technique used in data preprocessing to normalize the features of a dataset within a specific range. The goal is to scale the data so that it falls within a predefined interval, usually [0, 1]. This is achieved by transforming each data point $(x_i)$ in a feature to a new value $(x_i')$ using the following formula:\n",
        "\n",
        "$[ x_i' = \\frac{x_i - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} ]$\n",
        "where:\n",
        "- $(x_i)$ is an individual data point in the feature.\n",
        "- $(\\text{min}(X))$ is the minimum value of the feature.\n",
        "- $(\\text{max}(X))$ is the maximum value of the feature.\n",
        "\n",
        "This transformation ensures that the scaled values are in the desired range and preserves the relative relationships between different data points. Min-Max scaling is particularly useful when working with algorithms that are sensitive to the scale of the input features, such as support vector machines, k-nearest neighbors, and neural networks.\n",
        "\n",
        "Here's an example in Python using the popular scikit-learn library:\n",
        "\n"
      ],
      "metadata": {
        "id": "AyDbxB5i0hDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "data = np.array([[2.0, 5.0],\n",
        "                 [1.0, 8.0],\n",
        "                 [4.0, 3.0],\n",
        "                 [3.0, 9.0]])\n",
        "\n",
        "# Create a MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the data and transform it\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(data)\n",
        "print(\"\\nScaled data:\")\n",
        "print(scaled_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR62VYtd28Dt",
        "outputId": "25c0009d-6609-4190-ab7f-041c9e293018"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            "[[2. 5.]\n",
            " [1. 8.]\n",
            " [4. 3.]\n",
            " [3. 9.]]\n",
            "\n",
            "Scaled data:\n",
            "[[0.33333333 0.33333333]\n",
            " [0.         0.83333333]\n",
            " [1.         0.        ]\n",
            " [0.66666667 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
        "\n",
        "**A2.**\n",
        "\n",
        "\n",
        "The unit vector technique, also known as vector normalization or feature scaling by unit vector, involves scaling each data point in a feature by dividing it by the magnitude (length) of the vector formed by the feature values. This ensures that each data point lies on the surface of a unit hypersphere. The formula for unit vector scaling is as follows:\n",
        "\n",
        "$[ x_i' = \\frac{x_i}{\\|X\\|} ]$\n",
        "\n",
        "where:\n",
        "- $(x_i)$ is an individual data point in the feature.\n",
        "- $(X)$ is the vector of all data points in the feature.\n",
        "- $(\\|X|)$ is the magnitude (Euclidean norm) of the vector $(X)$.\n",
        "\n",
        "The unit vector technique normalizes the feature vectors, maintaining their direction and scaling them to have a magnitude of 1. Unlike Min-Max scaling, unit vector scaling does not necessarily confine the values to a specific range like [0, 1].\n",
        "\n",
        "Here's an example in Python to illustrate unit vector scaling using NumPy:\n",
        "\n"
      ],
      "metadata": {
        "id": "AGY5MXXt0iDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "data = np.array([[2.0, 5.0],\n",
        "                 [1.0, 8.0],\n",
        "                 [4.0, 3.0],\n",
        "                 [3.0, 9.0]])\n",
        "\n",
        "# Calculate the magnitude of each feature vector\n",
        "magnitude = np.linalg.norm(data, axis=1, keepdims=True)\n",
        "\n",
        "# Apply unit vector scaling\n",
        "unit_vector_scaled_data = data / magnitude\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(data)\n",
        "print(\"\\nUnit vector scaled data:\")\n",
        "print(unit_vector_scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHMuAHCu4JqY",
        "outputId": "f8e33e86-b875-41bf-e980-ce378197a1d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            "[[2. 5.]\n",
            " [1. 8.]\n",
            " [4. 3.]\n",
            " [3. 9.]]\n",
            "\n",
            "Unit vector scaled data:\n",
            "[[0.37139068 0.92847669]\n",
            " [0.12403473 0.99227788]\n",
            " [0.8        0.6       ]\n",
            " [0.31622777 0.9486833 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
        "example to illustrate its application.**\n",
        "\n",
        "**A3.**\n",
        "\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis. The goal of PCA is to transform the original features of a dataset into a new set of uncorrelated variables, called principal components, while retaining as much of the variance in the data as possible. This can help reduce the dimensionality of the data, making it more manageable and potentially improving the performance of machine learning models.\n",
        "\n",
        "The principal components are linear combinations of the original features, ordered by the amount of variance they capture. The first principal component accounts for the most variance, the second principal component for the second most, and so on.\n",
        "\n",
        "Here's a simplified overview of the steps involved in PCA:\n",
        "\n",
        "1.Standardize the Data: It's common practice to standardize the data (subtract the mean and divide by the standard deviation) so that all features have the same scale.\n",
        "\n",
        "2.Calculate the Covariance Matrix: Find the covariance matrix of the standardized data. The covariance matrix represents the relationships between all pairs of features.\n",
        "\n",
        "3.Calculate Eigenvectors and Eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of the variance in those directions.\n",
        "\n",
        "4.Sort Eigenvectors by Eigenvalues: Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue represents the first principal component.\n",
        "\n",
        "5.Select Principal Components: Choose the top $k$ eigenvectors to form the new feature space. The value of $k$ is determined based on the desired dimensionality of the reduced dataset.\n",
        "\n",
        "6.Project the Data: Project the original data onto the selected principal components to obtain the reduced-dimensional representation."
      ],
      "metadata": {
        "id": "Mm-UNZRK0iia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "columns = [f\"Feature {i+1}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Standardize the data\n",
        "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_standardized)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame(data=X_pca, columns=[\"Principal Component 1\", \"Principal Component 2\"])\n",
        "df[\"Target\"] = iris.target\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Data:\")\n",
        "print(pd.DataFrame(data=X, columns=columns).head())\n",
        "\n",
        "print(\"\\nPCA Results:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWdRcVO356i6",
        "outputId": "ddeb746c-19ef-4318-bfa0-d684df13bbf7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Feature 1  Feature 2  Feature 3  Feature 4\n",
            "0        5.1        3.5        1.4        0.2\n",
            "1        4.9        3.0        1.4        0.2\n",
            "2        4.7        3.2        1.3        0.2\n",
            "3        4.6        3.1        1.5        0.2\n",
            "4        5.0        3.6        1.4        0.2\n",
            "\n",
            "PCA Results:\n",
            "   Principal Component 1  Principal Component 2  Target\n",
            "0              -2.264703               0.480027       0\n",
            "1              -2.080961              -0.674134       0\n",
            "2              -2.364229              -0.341908       0\n",
            "3              -2.299384              -0.597395       0\n",
            "4              -2.389842               0.646835       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept.**\n",
        "\n",
        "**A4.**\n",
        "\n",
        "\n",
        "PCA (Principal Component Analysis) can be used for feature extraction in the context of dimensionality reduction. Feature extraction involves transforming the original features of a dataset into a new set of features, typically with the goal of reducing the dimensionality or capturing the most important information in the data. PCA achieves feature extraction by creating linear combinations of the original features, known as principal components.\n",
        "\n",
        "The relationship between PCA and feature extraction can be summarized as follows:\n",
        "\n",
        "1. **Dimensionality Reduction:** PCA is primarily used for dimensionality reduction. By selecting a subset of the principal components, you effectively extract a reduced set of features that still capture most of the variance in the data.\n",
        "\n",
        "2. **Uncorrelated Features:** The principal components generated by PCA are uncorrelated, meaning they are linearly independent. This can be beneficial for certain machine learning algorithms that assume feature independence.\n",
        "\n",
        "3. **Variance Retention:** PCA retains as much variance as possible in the data. The first few principal components often capture the majority of the variance, allowing for a lower-dimensional representation of the data.\n",
        "\n",
        "Here's an example using Python and scikit-learn to demonstrate PCA for feature extraction:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qPgnKmhz1QlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "columns = [f\"Feature {i+1}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Standardize the data\n",
        "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Apply PCA for feature extraction\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components for illustration\n",
        "X_pca = pca.fit_transform(X_standardized)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame(data=X_pca, columns=[\"Principal Component 1\", \"Principal Component 2\"])\n",
        "df[\"Target\"] = iris.target\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Data:\")\n",
        "print(pd.DataFrame(data=X, columns=columns).head())\n",
        "\n",
        "print(\"\\nPCA Results (Feature Extraction):\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_gsQdU1540z",
        "outputId": "cedcf616-338a-4416-c80f-2c0791275ae1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Feature 1  Feature 2  Feature 3  Feature 4\n",
            "0        5.1        3.5        1.4        0.2\n",
            "1        4.9        3.0        1.4        0.2\n",
            "2        4.7        3.2        1.3        0.2\n",
            "3        4.6        3.1        1.5        0.2\n",
            "4        5.0        3.6        1.4        0.2\n",
            "\n",
            "PCA Results (Feature Extraction):\n",
            "   Principal Component 1  Principal Component 2  Target\n",
            "0              -2.264703               0.480027       0\n",
            "1              -2.080961              -0.674134       0\n",
            "2              -2.364229              -0.341908       0\n",
            "3              -2.299384              -0.597395       0\n",
            "4              -2.389842               0.646835       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
        "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
        "preprocess the data.**\n",
        "\n",
        "**A5.**\n",
        "\n",
        "\n",
        "In the context of building a recommendation system for a food delivery service with features like price, rating, and delivery time, Min-Max scaling can be employed to preprocess the data. Min-Max scaling is a method used to normalize the range of independent variables or features of a dataset. It scales the data in a way that it falls within a specified range, often [0, 1].\n",
        "\n",
        "Here's a step-by-step explanation of how Min-Max scaling can be applied to your dataset:\n",
        "\n",
        "1. **Understand the Features:**\n",
        "   - Identify the features in your dataset that need to be scaled. In your case, this could include features such as price, rating, and delivery time.\n",
        "\n",
        "2. **Compute the Min and Max Values:**\n",
        "   - For each feature, calculate the minimum$( \\text{min}(X))$ and maximum $( \\text{max}(X) )$ values. This involves finding the minimum and maximum values for price, rating, and delivery time.\n",
        "\n",
        "3. **Apply Min-Max Scaling:**\n",
        "   - For each data point $( x_i )$ in a feature, apply the Min-Max scaling formula:\n",
        " $[ x_i' = \\frac{x_i - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} ]$\n",
        "\n",
        "     This formula scales each data point to a value between 0 and 1 based on the range of the original data.\n",
        "\n",
        "4. **Repeat for Each Feature:**\n",
        "   - Repeat the scaling process for each feature in your dataset.\n",
        "\n",
        "5. **Updated Scaled Dataset:**\n",
        "   - The result is a new dataset where each feature has been scaled using Min-Max scaling. The scaled features will now have values between 0 and 1, which can be particularly useful when dealing with features that have different units or scales.\n",
        "\n",
        "Here's a simple example in Python using the scikit-learn library:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GKaApXd31ZOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = pd.DataFrame({\n",
        "    'price': [10, 20, 15, 25],\n",
        "    'rating': [4.0, 4.5, 3.8, 4.2],\n",
        "    'delivery_time': [30, 45, 35, 50]\n",
        "})\n",
        "\n",
        "# Apply Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Create a DataFrame for the scaled data\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=data.columns)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Data:\")\n",
        "print(data)\n",
        "print(\"\\nMin-Max Scaled Data:\")\n",
        "print(scaled_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoMy64HJ6dPA",
        "outputId": "724bbdb8-e630-4cc7-ab54-8325351fce91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   price  rating  delivery_time\n",
            "0     10     4.0             30\n",
            "1     20     4.5             45\n",
            "2     15     3.8             35\n",
            "3     25     4.2             50\n",
            "\n",
            "Min-Max Scaled Data:\n",
            "      price    rating  delivery_time\n",
            "0  0.000000  0.285714           0.00\n",
            "1  0.666667  1.000000           0.75\n",
            "2  0.333333  0.000000           0.25\n",
            "3  1.000000  0.571429           1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
        "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
        "dimensionality of the dataset.**\n",
        "\n",
        "**A6.**\n",
        "\n",
        "\n",
        "When working on a project to predict stock prices with a dataset containing numerous features, PCA (Principal Component Analysis) can be a valuable tool for reducing the dimensionality of the data. Reducing dimensionality is important to handle the curse of dimensionality, improve model performance, and simplify the interpretation of results. Here's a step-by-step guide on how you might use PCA for dimensionality reduction in the context of predicting stock prices:\n",
        "\n",
        "1. **Understand the Features:**\n",
        "   - Identify the features in your dataset related to company financial data and market trends. This could include variables like revenue, earnings, market indices, and other financial indicators.\n",
        "\n",
        "2. **Standardize the Data:**\n",
        "   - Standardize the data by subtracting the mean and dividing by the standard deviation for each feature. Standardization is important for PCA, as it ensures that all features are on a comparable scale.\n",
        "\n",
        "3. **Apply PCA:**\n",
        "   - Use PCA to transform the standardized features into a set of linearly uncorrelated variables called principal components.\n",
        "   - Specify the number of components you want to retain based on the desired level of dimensionality reduction. You can choose a number that retains a significant amount of the variance in the data.\n",
        "\n",
        "4. **Evaluate Explained Variance:**\n",
        "   - Examine the explained variance ratio for each principal component. The explained variance ratio indicates the proportion of the total variance in the original data that is captured by each principal component.\n",
        "   - Decide on the number of principal components to retain based on the cumulative explained variance. You may choose a threshold (e.g., retaining 95% of the variance) or a specific number of components.\n",
        "\n",
        "5. **Transform the Data:**\n",
        "   - Project the original data onto the selected principal components to obtain a reduced-dimensional representation of the dataset.\n",
        "\n",
        "6. **Train Predictive Models:**\n",
        "   - Use the reduced-dimensional dataset to train your predictive models for stock price prediction. The reduced dataset often contains the most important information while having fewer features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ie5R89e_16gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
        "values to a range of -1 to 1.**\n",
        "\n",
        "**A7.**\n",
        "\n",
        "To perform Min-Max scaling and transform the values of a dataset to a range of -1 to 1, you can use the following formula:\n",
        "\n",
        "$[ x_i' = \\frac{2 \\times (x_i - \\text{min}(X))}{\\text{max}(X) - \\text{min}(X)} - 1 ]$\n",
        "\n",
        "Let's apply this formula to the dataset $([1, 5, 10, 15, 20])$:\n",
        "\n",
        "1. Find the minimum and maximum values:\n",
        "   $(\\text{min}(X) = 1)$ and $(\\text{max}(X) = 20)$\n",
        "\n",
        "2. Apply Min-Max scaling to each data point $(x_i)$:\n",
        "   $[ x_i' = \\frac{2 \\times (x_i - 1)}{20 - 1} - 1 ]$\n",
        "\n",
        "Here's how you can perform this calculation in Python:\n"
      ],
      "metadata": {
        "id": "BdtYPBDd13hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Original dataset\n",
        "data = np.array([1, 5, 10, 15, 20])\n",
        "\n",
        "# Calculate min and max\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "\n",
        "# Apply Min-Max scaling\n",
        "scaled_data = 2 * (data - min_value) / (max_value - min_value) - 1\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data)\n",
        "print(\"\\nMin-Max Scaled Data (in the range [-1, 1]):\")\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ilwZvOIBLM0",
        "outputId": "75af60f1-35dc-4307-d60f-beb6ac570f25"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[ 1  5 10 15 20]\n",
            "\n",
            "Min-Max Scaled Data (in the range [-1, 1]):\n",
            "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
        "Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
        "\n",
        "**A8.**\n",
        "The choice of the number of principal components to retain in PCA is typically based on the amount of variance you want to preserve in the data. A common approach is to consider the cumulative explained variance ratio, which indicates the proportion of the total variance in the original data that is captured by a certain number of principal components.\n",
        "\n",
        "Here's a step-by-step guide:\n",
        "\n",
        "1. **Standardize the Data:**\n",
        "   - Before applying PCA, it's essential to standardize the data. This involves subtracting the mean and dividing by the standard deviation for each feature.\n",
        "\n",
        "2. **Apply PCA:**\n",
        "   - Use PCA to transform the standardized features into principal components.\n",
        "\n",
        "3. **Calculate Explained Variance Ratio:**\n",
        "   - Examine the explained variance ratio for each principal component. The explained variance ratio $(EV_i)$ represents the proportion of the total variance explained by the $(i)th$ principal component.\n",
        "   - Calculate the cumulative explained variance ratio $(CEV_k)$ for the first $(k)$ principal components:\n",
        "  $[ CEV_k = \\frac{\\sum_{i=1}^{k} EV_i}{\\sum_{i=1}^{n} EV_i}]$\n",
        "     where $(n)$ is the total number of principal components.\n",
        "\n",
        "4. **Choose the Number of Components:**\n",
        "   - Decide on the number of principal components (\\(k\\)) to retain based on the cumulative explained variance ratio. A common threshold is to retain enough components to capture a high percentage of the total variance, such as 95% or 99%.\n",
        "\n",
        "Let's illustrate this with Python using scikit-learn:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qlz2PpQM2Mdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample dataset (replace with your actual data)\n",
        "data = [[170, 65, 25, 1, 120],\n",
        "        [160, 55, 30, 0, 110],\n",
        "        [180, 70, 35, 1, 130],\n",
        "        [165, 60, 28, 0, 115],\n",
        "        [175, 75, 40, 1, 125]]\n",
        "\n",
        "# Extract features (excluding the target variable)\n",
        "X = [row[:-1] for row in data]\n",
        "\n",
        "# Standardize the data\n",
        "X_standardized = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_standardized)\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "cumulative_explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Find the number of components to retain (e.g., 95% of the variance)\n",
        "num_components_to_retain = np.argmax(cumulative_explained_variance_ratio >= 0.95) + 1\n",
        "\n",
        "# Display the cumulative explained variance ratio\n",
        "print(\"Cumulative Explained Variance Ratio:\")\n",
        "print(cumulative_explained_variance_ratio)\n",
        "\n",
        "# Display the number of components to retain\n",
        "print(\"\\nNumber of Components to Retain:\")\n",
        "print(num_components_to_retain)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3y5K6RcCHrR",
        "outputId": "5d9b5cb9-6622-4e2b-c81a-c5a38c2e5366"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Explained Variance Ratio:\n",
            "[0.79975352 0.95954107 0.98767596 1.        ]\n",
            "\n",
            "Number of Components to Retain:\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eskUu6F-CIkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}