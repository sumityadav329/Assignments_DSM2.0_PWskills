{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54803eb1",
   "metadata": {},
   "source": [
    "# Q1:\n",
    "You are working on a machine learning project where you have a dataset containing numerical and\n",
    "categorical features. You have identified that some of the features are highly correlated and there are\n",
    "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps- \n",
    "- Use an automated feature selection method to identify the important features in the dataset\n",
    "- Create a numerical pipeline that includes the following steps\"\n",
    "- Impute the missing values in the numerical columns using the mean of the column values\n",
    "- Scale the numerical columns using standardisation\n",
    "- Create a categorical pipeline that includes the following steps\"\n",
    "- Impute the missing values in the categorical columns using the most frequent value of the column\n",
    "- One-hot encode the categorical columns\n",
    "- Combine the numerical and categorical pipelines using a ColumnTransformer\n",
    "- Use a Random Forest Classifier to build the final model\n",
    "- Evaluate the accuracy of the model on the test dataset\n",
    "\n",
    "Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
    "each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
    "the pipeline\n",
    "\n",
    "# A1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735576f6-6e2f-4c87-b744-d58a6ca4afc3",
   "metadata": {},
   "source": [
    "Let's use the `wine` dataset available in scikit-learn, which is a classification dataset, to create a comprehensive pipeline that handles feature engineering, model building, and evaluation.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Load the Wine Dataset**: Utilize the `load_wine` function from `sklearn.datasets`.\r\n",
    "2. **Split the Data**: Split the dataset into training and test sets.\r\n",
    "3. **Feature Engineering Pipeline**:\r\n",
    "    - **Numerical Pipeline**: Impute missing values and scale the numerical features.\r\n",
    "    - **Categorical Pipeline**: Since the Wine dataset mainly consists of numerical features, focus on the numerical pipeline.\r\n",
    "    - **Combine Pipelines**: Merge the numerical pipeline using `ColumnTransformer`.\r\n",
    "4. **Model Building**: Use `RandomForestClassifier` as our classification model.\r\n",
    "5. **Evaluate the Model**: Assess the model's performance on the test dataset using alild a robust classification model to predict wine types effectively.ngineering and model building while ensuring robust performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d39b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Feature Engineering Pipeline\n",
    "# Numerical Pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine pipelines using ColumnTransformer (only the numerical pipeline as the dataset is mainly numerical)\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, slice(0, X.shape[1]))  # All features are numerical in the Wine dataset\n",
    "])\n",
    "\n",
    "# Step 4: Model Building\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb78de0-8a0d-4ba7-a32f-db17bc6bda30",
   "metadata": {},
   "source": [
    "\n",
    "### Interpretation:\n",
    "- This pipeline automates the preprocessing steps for the Wine dataset, including imputation and scaling of numerical features.\n",
    "- The RandomForestClassifier is employed for classification, aiming to predict the type of wine based on its attributes.\n",
    "- The evaluation metric used here is accuracy, representing the proportion of correctly classified instances.\n",
    "\n",
    "### Possible Improvements:\n",
    "- **Hyperparameter Tuning**: Optimize hyperparameters for RandomForestClassifier using techniques like GridSearchCV or RandomizedSearchCV.\n",
    "- **Feature Importance Analysis**: Analyze feature importances to identify the most influential attributes in predicting the wine types.\n",
    "- **Additional Feature Engineering**: Explore other preprocessing techniques or algorithms that might further enhance model performance.\n",
    "\n",
    "By adapting this pipeline to the Wine dataset, you can automate the feature engineering process and build a robust classification model to predict wine types effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576c738",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "**Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
    "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
    "accuracy.**\n",
    "\n",
    "# A2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf569391-1050-4125-aa30-7f30d36ff2ff",
   "metadata": {},
   "source": [
    "we'll create a pipeline that includes both a `RandomForestClassifier` and a `LogisticRegression` classifier. Then, we'll use a `VotingClassifier` to combine their predictions. The model will be trained and evaluated on the famous Iris dataset.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "1. **Load the Iris Dataset**: Utilize the `load_iris` function from `sklearn.datasets`.\r\n",
    "2. **Split the Data**: Split the dataset into training and test sets.\r\n",
    "3. **Build the Pipeline**:\r\n",
    "    - Create individual pipelines for `RandomForestClassifier` and `LogisticRegression`.\r\n",
    "    - Use `VotingClassifier` to combine predictions from both classifiers.\r\n",
    "4. **Train and Evaluate the Model**: Fit the pipeline on the training data and evaluate its accuracy on the testy improving overall prediction performance on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6b63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Build the Pipeline\n",
    "# Create individual pipelines for RandomForestClassifier and LogisticRegression\n",
    "rf_pipeline = Pipeline([\n",
    "    ('rf_classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('lr_classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Combine pipelines using VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_pipeline), ('lr', lr_pipeline)],\n",
    "    voting='soft'  # 'soft' voting uses predicted class probabilities for averaging\n",
    ")\n",
    "\n",
    "# Create a main pipeline including the voting classifier\n",
    "pipeline = Pipeline([\n",
    "    ('voting_clf', voting_clf)\n",
    "])\n",
    "\n",
    "# Step 4: Train and Evaluate the Model\n",
    "# Fit the pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c583a-f6ea-4024-ad28-69e4b6f7ef63",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "- The pipeline combines predictions from both `RandomForestClassifier` and `LogisticRegression` models using a `VotingClassifier`.\n",
    "- By leveraging ensemble learning, the voting classifier aims to improve prediction accuracy by aggregating the predictions from multiple models.\n",
    "- The evaluation metric used here is accuracy, which represents the proportion of correctly classified instances in the test dataset.\n",
    "\n",
    "By following this approach, you can create an ensemble model that leverages the strengths of both RandomForest and LogisticRegression classifiers, potentially improving overall prediction performance on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4754c8-c9bc-436a-8649-a2477bef3eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
